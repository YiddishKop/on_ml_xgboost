#+TITLE: 3.6 Handle Imbalanced Datasets

* Handle Imbalanced Dataset

There are plenty of examples in real-world problems that deals with imbalanced
target classes. Imagine medical data where there are only a few positive
instances out of thousands of negatie (normal) ones. Another example might be
analyzing fraud transaction, in which the actual frauds represent only a
fraction of all available data.

#+BEGIN_QUOTE
Imbalanced data refers to a classification problems where the classes are not
equally distributed.
#+END_QUOTE

You can read good introduction about tackling imbalanced datasets [[http://machinelearningmastery.com/tactics-to-combat-imbalanced-classes-in-your-machine-learning-dataset/][here]].

#+BEGIN_EXAMPLE
**What you will learn**:
- what are the common approaches when dealing with class imbalance
- what is the *accuracy paradox*,
- how to manually denote which samples are more important than others
- how to use scale_pos_weight parameter to do it automatically
#+END_EXAMPLE

*** General advices
These are some common tactics when approaching imbalanced datasets:
- collect more data,
- use better evaluation metric (that notices mistakes - ie. AUC, F1, Kappa, ...),
- try oversampling minority class or undersampling majority class,
- generate artificial samples of minority class (ie. SMOTE algorithm)

In XGBoost you can try to:
- make sure that parameter `min_child_weight` is small (because leaf nodes can have smaller size groups), it is set to `min_child_weight=1` by default,
- assign more weights to specific samples while initalizing `DMatrix`,
- control the balance of positive and negative weights  using `set_pos_weight` parameter,
- use AUC for evaluation

*** Prepare data
Let's test it by generating an artificial dataset to perform some experiments.
But first load essential libraries that will be used throughout the lecture.


#+BEGIN_SRC ipython :session :exports code :async t :results raw drawer
  import numpy as np
  import pandas as pd

  import xgboost as xgb

  from sklearn.datasets import make_classification
  from sklearn.metrics import accuracy_score, precision_score, recall_score
  from sklearn.cross_validation import train_test_split

  # reproducibility
  seed = 123
#+END_SRC


We'll use a function to generate dataset for binary classification. To assure
that it's imbalanced use `weights` parameter. In this case there will be 200
samples each described by 5 features, but only 10% of them (about 20 samples)
will be positive. That makes the problem harder.


#+BEGIN_SRC ipython :session :exports code :async t :results raw drawer
  X, y = make_classification(
      n_samples=200,
      n_features=5,
      n_informative=3,
      n_classes=2,
      weights=[.9, .1],
      shuffle=True,
      random_state=seed
  )

  print('There are {} positive instances.'.format(y.sum()))
#+END_SRC


    There are 21 positive instances.


Divide created data into train and test. Remember so that both datasets should
be similiar in terms of distribution, so they need stratification.


#+BEGIN_SRC ipython :session :exports code :async t :results raw drawer
  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, stratify=y, random_state=seed)

  print('Total number of postivie train instances: {}'.format(y_train.sum()))
  print('Total number of positive test instances: {}'.format(y_test.sum()))
#+END_SRC


    Total number of postivie train instances: 14
    Total number of positive test instances: 7


*** Baseline model
In this approach try to completely ignore the fact that classed are imbalanced
and see how it will perform. Create `DMatrix` for train and test data.


#+BEGIN_SRC ipython :session :exports code :async t :results raw drawer
  dtrain = xgb.DMatrix(X_train, label=y_train)
  dtest = xgb.DMatrix(X_test)
#+END_SRC


Assume that we will create 15 decision tree stumps, solving binary
classification problem, where each next one will be train very aggressively.

These parameters will also be used in consecutive examples.


#+BEGIN_SRC ipython :session :exports code :async t :results raw drawer
  params = {
      'objective':'binary:logistic',
      'max_depth':1,
      'silent':1,
      'eta':1
  }

  num_rounds = 15
#+END_SRC


Train the booster and make predictions.


#+BEGIN_SRC ipython :session :exports code :async t :results raw drawer
  bst = xgb.train(params, dtrain, num_rounds)
  y_test_preds = (bst.predict(dtest) > 0.5).astype('int')
#+END_SRC


Let's see how the confusion matrix looks like.

#+BEGIN_SRC ipython :session :exports code :async t :results raw drawer
  pd.crosstab(
      pd.Series(y_test, name='Actual'),
      pd.Series(y_test_preds, name='Predicted'),
      margins=True
  )
#+END_SRC




#+BEGIN_SRC html
<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th>Predicted</th>
      <th>0</th>
      <th>1</th>
      <th>All</th>
    </tr>
    <tr>
      <th>Actual</th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>59</td>
      <td>0</td>
      <td>59</td>
    </tr>
    <tr>
      <th>1</th>
      <td>4</td>
      <td>3</td>
      <td>7</td>
    </tr>
    <tr>
      <th>All</th>
      <td>63</td>
      <td>3</td>
      <td>66</td>
    </tr>
  </tbody>
</table>
</div>
#+END_SRC



We can also present the performance using 3 different evaluation metrics:
- [accuracy](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html),
- [precision](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html) (the ability of the classifier not to label as positive a sample that is negative),
- [recall](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html) (the ability of the classifier to find all the positive samples).

#+BEGIN_SRC ipython :session :exports code :async t :results raw drawer
  print('Accuracy: {0:.2f}'.format(accuracy_score(y_test, y_test_preds)))
  print('Precision: {0:.2f}'.format(precision_score(y_test, y_test_preds)))
  print('Recall: {0:.2f}'.format(recall_score(y_test, y_test_preds)))
#+END_SRC


    Accuracy: 0.94
    Precision: 1.00
    Recall: 0.43


Intuitively we know that the foucs should be on finding positive samples. First
results are very promising (94% accuracy - wow), but deeper analysis show that
the results are biased towards majority class - we are very poor at predicting
the actual label of positive instances. That is called an [accuracy
paradox](https://en.wikipedia.org/wiki/Accuracy_paradox?oldformat=true).

*** Custom weights
Try to explicitly tell the algorithm what important using relative instance
weights. Let's specify that positive instances have 5x more weight and add this
information while creating `DMatrix`.


#+BEGIN_SRC ipython :session :exports code :async t :results raw drawer
  weights = np.zeros(len(y_train))
  weights[y_train == 0] = 1
  weights[y_train == 1] = 5

  dtrain = xgb.DMatrix(X_train, label=y_train, weight=weights) # weights added
  dtest = xgb.DMatrix(X_test)
#+END_SRC


Train the classifier and get predictions (same as in baseline):


#+BEGIN_SRC ipython :session :exports code :async t :results raw drawer
  bst = xgb.train(params, dtrain, num_rounds)
  y_test_preds = (bst.predict(dtest) > 0.5).astype('int')
#+END_SRC


Inspect the confusion matrix, and obtained evaluation metrics:


#+BEGIN_SRC ipython :session :exports code :async t :results raw drawer
  pd.crosstab(
      pd.Series(y_test, name='Actual'),
      pd.Series(y_test_preds, name='Predicted'),
      margins=True
  )
#+END_SRC

#+BEGIN_SRC html
<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th>Predicted</th>
      <th>0</th>
      <th>1</th>
      <th>All</th>
    </tr>
    <tr>
      <th>Actual</th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>52</td>
      <td>7</td>
      <td>59</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2</td>
      <td>5</td>
      <td>7</td>
    </tr>
    <tr>
      <th>All</th>
      <td>54</td>
      <td>12</td>
      <td>66</td>
    </tr>
  </tbody>
</table>
</div>

#+END_SRC



#+BEGIN_SRC ipython :session :exports code :async t :results raw drawer
  print('Accuracy: {0:.2f}'.format(accuracy_score(y_test, y_test_preds)))
  print('Precision: {0:.2f}'.format(precision_score(y_test, y_test_preds)))
  print('Recall: {0:.2f}'.format(recall_score(y_test, y_test_preds)))
#+END_SRC


    Accuracy: 0.86
    Precision: 0.42
    Recall: 0.71


You see that we made a trade-off here. We are now able to better classify the
minority class, but the overall accuracy and precision decreased. Test multiple
weights combinations and see which one works best.

*** Use `scale_pos_weight` parameter
You can automate the process of assigning weights manually by calculating the
proportion between negative and positive instances and setting it to
`scale_pos_weight` parameter.

Let's reinitialize datasets.


#+BEGIN_SRC ipython :session :exports code :async t :results raw drawer
  dtrain = xgb.DMatrix(X_train, label=y_train)
  dtest = xgb.DMatrix(X_test)
#+END_SRC


Calculate the ratio between both classes and assign it to a parameter.


#+BEGIN_SRC ipython :session :exports code :async t :results raw drawer
  train_labels = dtrain.get_label()

  ratio = float(np.sum(train_labels == 0)) / np.sum(train_labels == 1)
  params['scale_pos_weight'] = ratio
#+END_SRC


And like before, analyze the confusion matrix and obtained metrics
#+BEGIN_SRC ipython :session :exports code :async t :results raw drawer
bst = xgb.train(params, dtrain, num_rounds)
y_test_preds = (bst.predict(dtest) > 0.5).astype('int')

pd.crosstab(
    pd.Series(y_test, name='Actual'),
    pd.Series(y_test_preds, name='Predicted'),
    margins=True
)
#+END_SRC




#+BEGIN_SRC html
<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th>Predicted</th>
      <th>0</th>
      <th>1</th>
      <th>All</th>
    </tr>
    <tr>
      <th>Actual</th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>51</td>
      <td>8</td>
      <td>59</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0</td>
      <td>7</td>
      <td>7</td>
    </tr>
    <tr>
      <th>All</th>
      <td>51</td>
      <td>15</td>
      <td>66</td>
    </tr>
  </tbody>
</table>
</div>
#+END_SRC


#+BEGIN_SRC ipython :session :exports code :async t :results raw drawer
  print('Accuracy: {0:.2f}'.format(accuracy_score(y_test, y_test_preds)))
  print('Precision: {0:.2f}'.format(precision_score(y_test, y_test_preds)))
  print('Recall: {0:.2f}'.format(recall_score(y_test, y_test_preds)))
#+END_SRC


    Accuracy: 0.88
    Precision: 0.47
    Recall: 1.00


You can see that scalling weight by using `scale_pos_weights` in this case gives
better results that doing it manually. We are now able to perfectly classify all
posivie classes (focusing on the real problem). On the other hand the classifier
sometimes makes a mistake by wrongly classifing the negative case into positive
(producing so called *false positives*).
